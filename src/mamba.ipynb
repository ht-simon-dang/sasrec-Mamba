{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "# Hydra and OmegaConf\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# ClearML (optional, for experiment tracking)\n",
    "try:\n",
    "    from clearml import Task\n",
    "except ImportError:\n",
    "    Task = None\n",
    "    print(\"ClearML not found. To use ClearML, install with: pip install clearml\")\n",
    "\n",
    "\n",
    "# Attempt to import Mamba\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    print(\"WARNING: mamba_ssm not found. Using a placeholder Mamba module.\")\n",
    "    print(\"Please install mamba_ssm: pip install mamba-ssm causal-conv1d>=1.0.0\")\n",
    "    class Mamba(nn.Module):\n",
    "        def __init__(self, d_model, **kwargs):\n",
    "            super().__init__()\n",
    "            self.d_model = d_model\n",
    "            self.dummy_layer = nn.Linear(d_model, d_model)\n",
    "            print(\"Placeholder Mamba initialized. Model will not train correctly.\")\n",
    "        def forward(self, hidden_states, **kwargs):\n",
    "            return self.dummy_layer(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba4Rec(nn.Module):\n",
    "    \"\"\"\n",
    "    Mamba for Sequential Recommendation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, mamba_config, add_head=True,\n",
    "                 tie_weights=True, padding_idx=0, init_std=0.02,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the Mamba4Rec model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The total number of unique entities in the dataset\n",
    "                              (items + users + padding).\n",
    "            mamba_config (dict): Configuration dictionary for the Mamba model.\n",
    "                                 Must include 'd_model' (embedding/hidden size).\n",
    "            add_head (bool, optional): Whether to add a final linear layer. Defaults to True.\n",
    "            tie_weights (bool, optional): Whether to tie embedding and head weights. Defaults to True.\n",
    "            padding_idx (int, optional): Index for padding. Defaults to 0.\n",
    "            init_std (float, optional): Standard deviation for weight initialization. Defaults to 0.02.\n",
    "            **kwargs: Additional keyword arguments passed to the Mamba model constructor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.mamba_config = mamba_config\n",
    "        self.add_head = add_head\n",
    "        self.tie_weights = tie_weights\n",
    "        self.padding_idx = padding_idx\n",
    "        self.init_std = init_std\n",
    "\n",
    "        if 'd_model' not in mamba_config:\n",
    "            raise ValueError(\"mamba_config must contain 'd_model'\")\n",
    "        self.hidden_size = mamba_config['d_model']\n",
    "\n",
    "        self.embed_layer = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                        embedding_dim=self.hidden_size,\n",
    "                                        padding_idx=padding_idx)\n",
    "        self.mamba_model = Mamba(**mamba_config, **kwargs)\n",
    "\n",
    "        if self.add_head:\n",
    "            self.head = nn.Linear(self.hidden_size, vocab_size, bias=False)\n",
    "            if self.tie_weights:\n",
    "                if self.head.weight.shape != self.embed_layer.weight.shape:\n",
    "                     raise ValueError(f\"Head ({self.head.weight.shape}) and Embed ({self.embed_layer.weight.shape}) \"\n",
    "                                      f\"shapes don't match for tied weights.\")\n",
    "                self.head.weight = self.embed_layer.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initializes weights for embedding and head layers.\"\"\"\n",
    "        self.embed_layer.weight.data.normal_(mean=0.0, std=self.init_std)\n",
    "        if self.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embed_layer.weight.data[self.padding_idx].zero_()\n",
    "        if self.add_head and not self.tie_weights:\n",
    "             self.head.weight.data.normal_(mean=0.0, std=self.init_std)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the Mamba4Rec model.\n",
    "        `attention_mask` is included for API consistency but not used by Mamba.\n",
    "        \"\"\"\n",
    "        embeds = self.embed_layer(input_ids)\n",
    "        mamba_outputs = self.mamba_model(embeds)\n",
    "        outputs = mamba_outputs\n",
    "        if self.add_head:\n",
    "            outputs = self.head(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_time_idx(df, user_col='user_id', timestamp_col='timestamp', sort=True):\n",
    "    \"\"\"Add time index (0-based) to interactions dataframe.\"\"\"\n",
    "    if sort:\n",
    "        print(f\"Sorting interactions by {user_col} and {timestamp_col}...\")\n",
    "        df = df.sort_values([user_col, timestamp_col])\n",
    "    print(\"Adding time indices (time_idx and time_idx_reversed)...\")\n",
    "    df['time_idx'] = df.groupby(user_col).cumcount()\n",
    "    df['time_idx_reversed'] = df.groupby(user_col).cumcount(ascending=False)\n",
    "    return df\n",
    "\n",
    "def filter_items(df, item_min_count, user_col='user_id', item_col='item_id'):\n",
    "    \"\"\"Filter out items with fewer than item_min_count interactions.\"\"\"\n",
    "    print(f\"Filtering items with less than {item_min_count} unique user interactions...\")\n",
    "    item_counts = df.groupby(item_col)[user_col].nunique()\n",
    "    valid_items = item_counts[item_counts >= item_min_count].index\n",
    "    n_items_before = df[item_col].nunique()\n",
    "    df_filtered = df[df[item_col].isin(valid_items)].copy()\n",
    "    print(f\"  Items before: {n_items_before}, After: {df_filtered[item_col].nunique()}\")\n",
    "    return df_filtered\n",
    "\n",
    "def filter_users(df, user_min_count, user_col='user_id', item_col='item_id'):\n",
    "    \"\"\"Filter out users with fewer than user_min_count interactions.\"\"\"\n",
    "    print(f\"Filtering users with less than {user_min_count} unique item interactions...\")\n",
    "    user_counts = df.groupby(user_col)[item_col].nunique()\n",
    "    valid_users = user_counts[user_counts >= user_min_count].index\n",
    "    n_users_before = df[user_col].nunique()\n",
    "    df_filtered = df[df[user_col].isin(valid_users)].copy()\n",
    "    print(f\"  Users before: {n_users_before}, After: {df_filtered[user_col].nunique()}\")\n",
    "    return df_filtered\n",
    "\n",
    "def map_ids(df, user_col='user_id', item_col='item_id'):\n",
    "    \"\"\"\n",
    "    Maps original user_ids and item_ids to contiguous integer ranges.\n",
    "    Item IDs: 1 to N. User IDs: N+1 to N+M. 0 is for padding.\n",
    "    \"\"\"\n",
    "    print(\"Mapping original IDs to contiguous integer ranges...\")\n",
    "    unique_items = df[item_col].unique()\n",
    "    item_map = {item_id: i + 1 for i, item_id in enumerate(unique_items)}\n",
    "    num_items = len(item_map)\n",
    "    df['mapped_item_id'] = df[item_col].map(item_map)\n",
    "    print(f\"  Mapped {num_items} unique items (IDs 1 to {num_items}).\")\n",
    "\n",
    "    unique_users = df[user_col].unique()\n",
    "    # User IDs start after the last item ID\n",
    "    user_map = {user_id: i + num_items + 1 for i, user_id in enumerate(unique_users)}\n",
    "    num_users = len(user_map)\n",
    "    df['mapped_user_id'] = df[user_col].map(user_map)\n",
    "    print(f\"  Mapped {num_users} unique users (IDs {num_items + 1} to {num_items + num_users}).\")\n",
    "\n",
    "    # Total entities for vocab size: items + users + 1 (for padding token 0)\n",
    "    total_entities_in_vocab = num_items + num_users + 1\n",
    "    print(f\"  Total entities for vocab (incl. padding=0): {total_entities_in_vocab}\")\n",
    "    return df, user_map, item_map, num_items, num_users, total_entities_in_vocab\n",
    "\n",
    "def create_sequences_with_user_id(df, user_col='user_id', mapped_item_col='mapped_item_id', mapped_user_col='mapped_user_id'):\n",
    "    \"\"\"\n",
    "    Generates sequences for each user, inserting the user's mapped ID\n",
    "    after every two mapped item IDs. Assumes df is sorted by user and timestamp.\n",
    "    \"\"\"\n",
    "    print(\"Generating sequences with user ID injection...\")\n",
    "    user_sequences = defaultdict(list)\n",
    "    grouped = df.groupby(user_col)\n",
    "    for user_id, group in grouped:\n",
    "        item_sequence = group[mapped_item_col].tolist()\n",
    "        mapped_user_id_val = group[mapped_user_col].iloc[0]\n",
    "        new_sequence = []\n",
    "        item_count_since_last_user = 0\n",
    "        for item_id_val in item_sequence:\n",
    "            new_sequence.append(item_id_val)\n",
    "            item_count_since_last_user += 1\n",
    "            if item_count_since_last_user == 2:\n",
    "                new_sequence.append(mapped_user_id_val)\n",
    "                item_count_since_last_user = 0\n",
    "        user_sequences[user_id] = new_sequence\n",
    "    print(f\"Finished generating sequences for {len(user_sequences)} users.\")\n",
    "    return dict(user_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preds_to_item_recs_mixed_vocab(predictions_batches, item_id_reverse_map, num_items_in_vocab, top_k_items_to_return=10):\n",
    "    \"\"\"\n",
    "    Processes raw model predictions (mixed vocab) to generate item recommendations.\n",
    "    \"\"\"\n",
    "    print(f\"Starting post-processing of prediction batches for top {top_k_items_to_return} items...\")\n",
    "    user_recs_list = []\n",
    "    for batch_idx, batch_output in enumerate(predictions_batches):\n",
    "        original_user_ids_batch = batch_output['user_ids']\n",
    "        predicted_entity_ids_batch = batch_output['preds'] # Mapped entity IDs\n",
    "        predicted_entity_scores_batch = batch_output['scores']\n",
    "\n",
    "        for i in range(len(original_user_ids_batch)):\n",
    "            user_id = original_user_ids_batch[i]\n",
    "            entity_ids_for_user = predicted_entity_ids_batch[i]\n",
    "            entity_scores_for_user = predicted_entity_scores_batch[i]\n",
    "\n",
    "            # Filter for actual items: IDs from 1 to num_items_in_vocab\n",
    "            item_mask = (entity_ids_for_user >= 1) & (entity_ids_for_user <= num_items_in_vocab)\n",
    "            actual_item_ids = entity_ids_for_user[item_mask]\n",
    "            actual_item_scores = entity_scores_for_user[item_mask]\n",
    "\n",
    "            if len(actual_item_ids) == 0:\n",
    "                continue\n",
    "\n",
    "            sorted_indices = np.argsort(actual_item_scores)[::-1]\n",
    "            top_items_for_user = actual_item_ids[sorted_indices][:top_k_items_to_return]\n",
    "            top_scores_for_user = actual_item_scores[sorted_indices][:top_k_items_to_return]\n",
    "\n",
    "            for item_idx in range(len(top_items_for_user)):\n",
    "                user_recs_list.append({\n",
    "                    'user_id': user_id,\n",
    "                    'mapped_item_id': top_items_for_user[item_idx],\n",
    "                    'prediction_score': top_scores_for_user[item_idx]\n",
    "                })\n",
    "\n",
    "    if not user_recs_list:\n",
    "        print(\"No valid item recommendations generated.\")\n",
    "        return pd.DataFrame(columns=['user_id', 'item_id', 'prediction_score'])\n",
    "\n",
    "    recs_df = pd.DataFrame(user_recs_list)\n",
    "    if item_id_reverse_map is not None:\n",
    "        recs_df['item_id'] = recs_df['mapped_item_id'].map(item_id_reverse_map)\n",
    "        recs_df.dropna(subset=['item_id'], inplace=True) # Drop if mapping failed\n",
    "        if 'item_id' in recs_df.columns and not recs_df['item_id'].empty:\n",
    "             # Attempt to cast to original item ID type, fallback to object\n",
    "            try:\n",
    "                original_item_type = pd.Series(list(item_id_reverse_map.values())).dtype\n",
    "                recs_df['item_id'] = recs_df['item_id'].astype(original_item_type)\n",
    "            except Exception:\n",
    "                recs_df['item_id'] = recs_df['item_id'].astype(object)\n",
    "\n",
    "    else:\n",
    "        recs_df['item_id'] = recs_df['mapped_item_id'] # Use mapped ID if no reverse map\n",
    "\n",
    "    if 'item_id' not in recs_df.columns: # Ensure column exists\n",
    "        recs_df['item_id'] = np.nan\n",
    "\n",
    "    final_df = recs_df[['user_id', 'item_id', 'prediction_score']].copy()\n",
    "    final_df.sort_values(['user_id', 'prediction_score'], ascending=[True, False], inplace=True)\n",
    "    print(f\"Post-processing complete. Returning {len(final_df)} item recommendations.\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysDataset(Dataset):\n",
    "    def __init__(self, sequences_dict, max_length, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences_dict (dict): Dict mapping original user_id to their sequence of mapped entity IDs.\n",
    "            max_length (int): Maximum sequence length for padding.\n",
    "            padding_idx (int): Value to use for padding.\n",
    "        \"\"\"\n",
    "        self.user_ids_orig = list(sequences_dict.keys())\n",
    "        self.sequences = [sequences_dict[uid] for uid in self.user_ids_orig]\n",
    "        self.max_length = max_length\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids_orig)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        \n",
    "        # Input sequence is up to max_length - 1, target is shifted\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "\n",
    "        # Pad sequences\n",
    "        input_len = len(input_seq)\n",
    "        target_len = len(target_seq)\n",
    "\n",
    "        padded_input = input_seq[:self.max_length] + [self.padding_idx] * (self.max_length - input_len)\n",
    "        padded_target = target_seq[:self.max_length] + [self.padding_idx] * (self.max_length - target_len)\n",
    "        \n",
    "        return {\n",
    "            'user_id_orig': self.user_ids_orig[idx], # For tracking/prediction\n",
    "            'input_ids': torch.LongTensor(padded_input[:self.max_length]),\n",
    "            'target_ids': torch.LongTensor(padded_target[:self.max_length])\n",
    "        }\n",
    "\n",
    "class PaddingCollateFn:\n",
    "    def __init__(self, padding_idx=0):\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Batch is a list of dicts from RecSysDataset.__getitem__\n",
    "        user_ids_orig = [item['user_id_orig'] for item in batch]\n",
    "        input_ids_list = [item['input_ids'] for item in batch]\n",
    "        target_ids_list = [item['target_ids'] for item in batch]\n",
    "\n",
    "        # Pad to the max length in this specific batch (already done in Dataset, but good for safety)\n",
    "        # Or rely on max_length from Dataset for consistent tensor shapes\n",
    "        padded_input_ids = torch.stack(input_ids_list)\n",
    "        padded_target_ids = torch.stack(target_ids_list)\n",
    "        \n",
    "        return {\n",
    "            'user_ids_orig': user_ids_orig,\n",
    "            'input_ids': padded_input_ids,\n",
    "            'target_ids': padded_target_ids\n",
    "        }\n",
    "\n",
    "class RecSysPredictionDataset(Dataset):\n",
    "    def __init__(self, sequences_dict, max_length, padding_idx=0):\n",
    "        \"\"\" For generating predictions, we only need the input sequence. \"\"\"\n",
    "        self.user_ids_orig = list(sequences_dict.keys())\n",
    "        # Use the full sequence as input for prediction\n",
    "        self.sequences = [sequences_dict[uid] for uid in self.user_ids_orig]\n",
    "        self.max_length = max_length\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_ids_orig)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        seq_len = len(seq)\n",
    "        padded_seq = seq[:self.max_length] + [self.padding_idx] * (self.max_length - seq_len)\n",
    "        \n",
    "        return {\n",
    "            'user_id_orig': self.user_ids_orig[idx],\n",
    "            'input_ids': torch.LongTensor(padded_seq[:self.max_length])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqRecModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate, predict_top_k=10, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.predict_top_k = predict_top_k\n",
    "        self.padding_idx = padding_idx # For loss calculation\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.padding_idx)\n",
    "        self.save_hyperparameters(ignore=['model']) # ignore model to avoid saving it twice\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        target_ids = batch['target_ids']\n",
    "        logits = self(input_ids) # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss: (N, C) and (N)\n",
    "        # Logits: (batch_size * seq_len, vocab_size)\n",
    "        # Target: (batch_size * seq_len)\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        target_ids = batch['target_ids']\n",
    "        logits = self(input_ids)\n",
    "        loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        \n",
    "        # Simple accuracy (next item prediction accuracy)\n",
    "        # Consider only non-padded targets\n",
    "        mask = (target_ids != self.padding_idx).view(-1)\n",
    "        if mask.sum() > 0:\n",
    "            preds = torch.argmax(logits.view(-1, logits.size(-1))[mask], dim=1)\n",
    "            correct_targets = target_ids.view(-1)[mask]\n",
    "            accuracy = (preds == correct_targets).float().mean()\n",
    "            self.log('val_acc', accuracy, prog_bar=True)\n",
    "        else:\n",
    "            self.log('val_acc', 0.0, prog_bar=True) # Or handle as NaN/skip\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        input_ids = batch['input_ids'] # (batch_size, seq_len)\n",
    "        user_ids_orig = batch['user_ids_orig']\n",
    "\n",
    "        logits = self(input_ids) # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        # For recommendation, typically use logits from the last relevant time step.\n",
    "        # Here, we'll take the logits corresponding to the prediction for the item *after* the last input item.\n",
    "        # This means we look at the logits at the sequence position of the last actual input item.\n",
    "        # A simpler approach for next-item prediction is to use the logits from the last time step of the output.\n",
    "        last_step_logits = logits[:, -1, :] # (batch_size, vocab_size)\n",
    "\n",
    "        scores, top_k_preds = torch.topk(last_step_logits, self.predict_top_k, dim=1)\n",
    "        \n",
    "        return {'user_ids': np.array(user_ids_orig), 'preds': top_k_preds.cpu().numpy(), 'scores': scores.cpu().numpy()}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_for_run(config: DictConfig):\n",
    "    \"\"\"Loads, preprocesses, and splits data.\"\"\"\n",
    "    print(\"--- Preparing Data ---\")\n",
    "    if config.data.path == \"dummy\":\n",
    "        print(\"Using dummy data...\")\n",
    "        data_dict = {\n",
    "            'user_id': ['u1','u1','u1','u1','u1', 'u2','u2','u2', 'u3','u3','u3','u3', 'u4','u4','u4','u4','u4'],\n",
    "            'item_id': ['i1','i2','i3','i4','i5', 'i2','i3','i6', 'i1','i7','i3','i8', 'i2','i5','i9','i1','i2'],\n",
    "            'timestamp': [1,  2,  3,  4,  5,   1,  2,  3,   1,  2,  3,  4,   1,  2,  3,  4,  5]\n",
    "        }\n",
    "        df = pd.DataFrame(data_dict)\n",
    "    else:\n",
    "        print(f\"Loading data from {config.data.path}...\")\n",
    "        # Assuming CSV with 'user_id', 'item_id', 'timestamp' columns\n",
    "        df = pd.read_csv(config.data.path) # Add sep, header etc. if needed\n",
    "\n",
    "    df = add_time_idx(df.copy()) # Ensure sorting and time_idx\n",
    "\n",
    "    # Apply filters if specified\n",
    "    if config.data.get('item_min_count', 0) > 0:\n",
    "        df = filter_items(df, config.data.item_min_count)\n",
    "    if config.data.get('user_min_count', 0) > 0:\n",
    "        df = filter_users(df, config.data.user_min_count)\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame is empty after filtering. Check filter conditions or data.\")\n",
    "\n",
    "    # Map IDs and create sequences (this is done for the whole dataset first)\n",
    "    df_mapped, user_map, item_map, num_items, num_users, total_entities_vocab = map_ids(df)\n",
    "    \n",
    "    # Create sequences based on the whole mapped df\n",
    "    # The sequences are then split logically for train/val/test\n",
    "    # This approach differs slightly from example, where df is split first\n",
    "    # For sequence-aware splitting, it's often better to split user sequences\n",
    "    \n",
    "    all_sequences_mapped = create_sequences_with_user_id(df_mapped.sort_values(['user_id', 'time_idx']))\n",
    "\n",
    "    # Split sequences for train, validation, test\n",
    "    # Example: for each user, last 2 interactions for test, next last for val\n",
    "    train_sequences, val_sequences, test_sequences = {}, {}, {}\n",
    "    for user_id_orig, seq in all_sequences_mapped.items():\n",
    "        if len(seq) < 3: # Need at least 3 items in mixed seq for train/val/test split\n",
    "            # Or put very short sequences only in train\n",
    "            train_sequences[user_id_orig] = seq \n",
    "            continue\n",
    "        \n",
    "        # This splitting needs care due to injected user IDs\n",
    "        # A simpler split based on original item interactions might be more robust\n",
    "        # For now, a simple split of the *generated mixed sequence*:\n",
    "        test_sequences[user_id_orig] = seq # The full sequence for predicting the very last part\n",
    "        val_sequences[user_id_orig] = seq[:-1] # Sequence up to second to last element\n",
    "        train_sequences[user_id_orig] = seq[:-2] # Sequence up to third to last\n",
    "    \n",
    "    # Filter out empty sequences after splitting\n",
    "    train_sequences = {k: v for k, v in train_sequences.items() if len(v) > 1} # Min len 2 for input/target pair\n",
    "    val_sequences = {k: v for k, v in val_sequences.items() if len(v) > 1}\n",
    "    test_sequences = {k: v for k, v in test_sequences.items() if len(v) > 0}\n",
    "\n",
    "\n",
    "    print(f\"  Num train user sequences: {len(train_sequences)}\")\n",
    "    print(f\"  Num val user sequences: {len(val_sequences)}\")\n",
    "    print(f\"  Num test user sequences: {len(test_sequences)}\")\n",
    "    \n",
    "    return train_sequences, val_sequences, test_sequences, item_map, user_map, num_items, num_users, total_entities_vocab\n",
    "\n",
    "\n",
    "def create_dataloaders_for_run(train_seq, val_seq, config: DictConfig, padding_idx=0):\n",
    "    print(\"--- Creating DataLoaders ---\")\n",
    "    train_dataset = RecSysDataset(train_seq, config.model.max_length, padding_idx)\n",
    "    val_dataset = RecSysDataset(val_seq, config.model.max_length, padding_idx)\n",
    "    \n",
    "    collate_fn = PaddingCollateFn(padding_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.trainer.batch_size, shuffle=True,\n",
    "                              num_workers=config.trainer.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "    eval_loader = DataLoader(val_dataset, batch_size=config.trainer.batch_size, shuffle=False,\n",
    "                             num_workers=config.trainer.num_workers, collate_fn=collate_fn, pin_memory=True)\n",
    "    print(f\"  Train Dataloader: {len(train_loader)} batches\")\n",
    "    print(f\"  Validation Dataloader: {len(eval_loader)} batches\")\n",
    "    return train_loader, eval_loader, train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def create_mamba_model_for_run(config: DictConfig, vocab_size: int, padding_idx: int):\n",
    "    print(\"--- Creating Model ---\")\n",
    "    # Ensure model_params from config are passed correctly\n",
    "    mamba_specific_config = config.model.model_params.mamba_config\n",
    "    \n",
    "    model = Mamba4Rec(\n",
    "        vocab_size=vocab_size,\n",
    "        mamba_config=mamba_specific_config, # Pass the nested mamba_config\n",
    "        padding_idx=padding_idx,\n",
    "        add_head=config.model.model_params.get('add_head', True),\n",
    "        tie_weights=config.model.model_params.get('tie_weights', True),\n",
    "        init_std=config.model.model_params.get('init_std', 0.02)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def training_run(seq_rec_module: SeqRecModule, train_loader: DataLoader, eval_loader: DataLoader, config: DictConfig):\n",
    "    print(\"--- Training Model ---\")\n",
    "    callbacks = [\n",
    "        TQDMProgressBar(refresh_rate=10), # Smaller refresh rate for faster updates\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=1,\n",
    "            monitor=config.trainer.early_stopping.monitor,\n",
    "            mode=config.trainer.early_stopping.mode,\n",
    "            filename='best_model-{epoch:02d}-{val_loss:.2f}'\n",
    "        )\n",
    "    ]\n",
    "    if config.trainer.early_stopping.enabled:\n",
    "        callbacks.append(EarlyStopping(\n",
    "            monitor=config.trainer.early_stopping.monitor,\n",
    "            patience=config.trainer.early_stopping.patience,\n",
    "            mode=config.trainer.early_stopping.mode,\n",
    "            verbose=True\n",
    "        ))\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.trainer.max_epochs,\n",
    "        accelerator=config.trainer.accelerator,\n",
    "        devices=config.trainer.devices,\n",
    "        callbacks=callbacks,\n",
    "        logger=True, # Basic logger, replace with ClearMLLogger or TensorBoardLogger if needed\n",
    "        deterministic=config.trainer.get('deterministic', False) # For reproducibility\n",
    "    )\n",
    "    trainer.fit(model=seq_rec_module, train_dataloaders=train_loader, val_dataloaders=eval_loader)\n",
    "    \n",
    "    # Load best model for prediction\n",
    "    best_model_path = callbacks[1].best_model_path # ModelCheckpoint is the second callback\n",
    "    print(f\"Best model path: {best_model_path}\")\n",
    "    if best_model_path:\n",
    "         # The LightningModule's load_from_checkpoint will recreate the module with its hparams\n",
    "        trained_module = SeqRecModule.load_from_checkpoint(best_model_path)\n",
    "    else:\n",
    "        print(\"No best model path found, using model from last epoch.\")\n",
    "        trained_module = seq_rec_module # Fallback\n",
    "        \n",
    "    return trainer, trained_module\n",
    "\n",
    "\n",
    "def predict_run(trainer: pl.Trainer, seq_rec_module: SeqRecModule, sequences_to_predict: dict, config: DictConfig, padding_idx: int):\n",
    "    print(\"--- Predicting ---\")\n",
    "    if not sequences_to_predict:\n",
    "        print(\"No sequences to predict. Skipping.\")\n",
    "        return pd.DataFrame(), None\n",
    "\n",
    "    predict_dataset = RecSysPredictionDataset(sequences_to_predict, config.model.max_length, padding_idx)\n",
    "    collate_fn = PaddingCollateFn(padding_idx=padding_idx) # Use the same collate for prediction inputs\n",
    "    \n",
    "    # Create a predict_loader that only provides input_ids and user_ids_orig\n",
    "    predict_loader_custom = DataLoader(\n",
    "        predict_dataset,\n",
    "        batch_size=config.trainer.batch_size, # Use same batch size or a specific predict_batch_size\n",
    "        shuffle=False,\n",
    "        num_workers=config.trainer.num_workers,\n",
    "        collate_fn=lambda batch: { # Custom collate for prediction\n",
    "            'user_ids_orig': [item['user_id_orig'] for item in batch],\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in batch])\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    seq_rec_module.model.eval() # Ensure model is in eval mode\n",
    "    seq_rec_module.predict_top_k = config.evaluation.get('predict_top_k_entities', 20) # How many entities model predicts\n",
    "    \n",
    "    raw_predictions_batches = trainer.predict(model=seq_rec_module, dataloaders=predict_loader_custom)\n",
    "    \n",
    "    return raw_predictions_batches, predict_dataset\n",
    "\n",
    "\n",
    "def evaluate_run(raw_predictions_batches: list, item_id_reverse_map: dict, num_items: int,\n",
    "                 ground_truth_sequences: dict, config: DictConfig, clearml_task: Task = None, prefix: str = \"eval\"):\n",
    "    print(f\"--- Evaluating {prefix} ---\")\n",
    "    if not raw_predictions_batches or not ground_truth_sequences:\n",
    "        print(f\"No predictions or ground truth for {prefix}. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    recs_df = preds_to_item_recs_mixed_vocab(\n",
    "        predictions_batches=raw_predictions_batches,\n",
    "        item_id_reverse_map=item_id_reverse_map,\n",
    "        num_items_in_vocab=num_items,\n",
    "        top_k_items_to_return=max(config.evaluation.top_k_metrics) # Get enough items for all K\n",
    "    )\n",
    "    print(f\"{prefix} recommendations head:\\n{recs_df.head()}\")\n",
    "\n",
    "    # Dummy metric calculation (replace with actual metric functions)\n",
    "    # Example: compute_metrics(test_df_for_eval, recs_df, k)\n",
    "    # test_df_for_eval needs to be created from ground_truth_sequences\n",
    "    # For simplicity, just printing shapes and a placeholder\n",
    "    \n",
    "    all_metrics = {}\n",
    "    for k_metric in config.evaluation.top_k_metrics:\n",
    "        # Placeholder for actual metric calculation.\n",
    "        # You'd typically compare `recs_df` against the held-out items in `ground_truth_sequences`.\n",
    "        # For example, for each user in `recs_df`, find their actual next items from `ground_truth_sequences`.\n",
    "        # Then calculate NDCG@k, Recall@k, etc.\n",
    "        \n",
    "        # Example of how ground truth for evaluation could be structured:\n",
    "        # gt_eval_data = []\n",
    "        # for user_id_orig, full_seq in ground_truth_sequences.items():\n",
    "        #     # Assuming the last item(s) of full_seq are the ground truth for prediction\n",
    "        #     # This depends on how train/val/test sequences were split.\n",
    "        #     # If test_sequences are full sequences, target is the very last item.\n",
    "        #     # If train_sequences were seq[:-1] and test_sequences were seq, then target is seq[-1]\n",
    "        #     # This part needs careful alignment with the data splitting logic.\n",
    "        #     # For now, let's assume ground_truth_sequences contain the items to be predicted.\n",
    "        #\n",
    "        #     # This is a simplified placeholder. True evaluation is complex.\n",
    "        #     # We need to extract the *actual next items* that were held out for this user.\n",
    "        #     # The current `ground_truth_sequences` are the *input* sequences for prediction.\n",
    "        #     # We need the corresponding *target* sequences that were held out.\n",
    "        #\n",
    "        #     # Let's assume for this placeholder that we have a separate ground truth DataFrame.\n",
    "        #     # For now, we'll just simulate some metrics.\n",
    "        \n",
    "        simulated_ndcg = np.random.rand() * 0.1 + (0.1 / k_metric) # Dummy value\n",
    "        simulated_recall = np.random.rand() * 0.2 + (0.2 / k_metric) # Dummy value\n",
    "        \n",
    "        metrics = {f'{prefix}_ndcg@{k_metric}': simulated_ndcg, f'{prefix}_recall@{k_metric}': simulated_recall}\n",
    "        print(metrics)\n",
    "        all_metrics.update(metrics)\n",
    "\n",
    "    if clearml_task:\n",
    "        for key, value in all_metrics.items():\n",
    "            clearml_task.get_logger().report_scalar(title=key, series=key, value=value, iteration=0)\n",
    "        print(f\"Metrics reported to ClearML for {prefix}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(version_base=None, config_path=None, config_name=\"experiment_config\")\n",
    "def run_experiment(config: DictConfig):\n",
    "    print(\"--- Experiment Start ---\")\n",
    "    print(OmegaConf.to_yaml(config))\n",
    "    \n",
    "    # For reproducibility\n",
    "    if config.get('seed'):\n",
    "        pl.seed_everything(config.seed, workers=True)\n",
    "\n",
    "    # Initialize ClearML Task (optional)\n",
    "    clearml_task_instance = None\n",
    "    if Task and config.clearml.project_name and config.clearml.task_name:\n",
    "        clearml_task_instance = Task.init(\n",
    "            project_name=config.clearml.project_name,\n",
    "            task_name=config.clearml.task_name,\n",
    "            reuse_last_task_id=False\n",
    "        )\n",
    "        clearml_task_instance.connect(OmegaConf.to_container(config, resolve=True))\n",
    "        print(\"ClearML task initialized.\")\n",
    "\n",
    "    # 1. Prepare Data\n",
    "    train_seqs, val_seqs, test_seqs, item_map, user_map, num_items, num_users, total_vocab_size = prepare_data_for_run(config)\n",
    "    item_id_reverse_map = {v: k for k, v in item_map.items()}\n",
    "    padding_idx = config.model.get('padding_idx', 0)\n",
    "\n",
    "    # 2. Create DataLoaders\n",
    "    train_loader, val_loader, train_dataset, val_dataset = create_dataloaders_for_run(train_seqs, val_seqs, config, padding_idx)\n",
    "\n",
    "    # 3. Create Model\n",
    "    mamba_model = create_mamba_model_for_run(config, total_vocab_size, padding_idx)\n",
    "    \n",
    "    # 4. Setup Lightning Module\n",
    "    seq_rec_lightning_module = SeqRecModule(\n",
    "        mamba_model,\n",
    "        learning_rate=config.trainer.learning_rate,\n",
    "        padding_idx=padding_idx\n",
    "    )\n",
    "    \n",
    "    # 5. Training\n",
    "    start_time = time.time()\n",
    "    trainer, trained_seq_rec_module = training_run(seq_rec_lightning_module, train_loader, val_loader, config)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "    if clearml_task_instance:\n",
    "        clearml_task_instance.get_logger().report_single_value('training_time_seconds', training_time)\n",
    "\n",
    "    # 6. Prediction & Evaluation on Validation Set (Optional, usually done during training by PL)\n",
    "    # Here we demonstrate explicit prediction and evaluation on the val set if needed post-training.\n",
    "    # val_raw_preds, _ = predict_run(trainer, trained_seq_rec_module, val_seqs, config, padding_idx)\n",
    "    # evaluate_run(val_raw_preds, item_id_reverse_map, num_items, val_seqs, config, clearml_task_instance, prefix=\"val_post_train\")\n",
    "\n",
    "    # 7. Prediction & Evaluation on Test Set\n",
    "    test_raw_preds, _ = predict_run(trainer, trained_seq_rec_module, test_seqs, config, padding_idx)\n",
    "    evaluate_run(test_raw_preds, item_id_reverse_map, num_items, test_seqs, config, clearml_task_instance, prefix=\"test\")\n",
    "\n",
    "    if clearml_task_instance:\n",
    "        clearml_task_instance.close()\n",
    "    print(\"--- Experiment End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dummy_config():\n",
    "    # This function creates a dummy config file if one doesn't exist.\n",
    "    # In a real Hydra setup, you'd have this in your config directory.\n",
    "    config_content = \"\"\"\n",
    "# @package _group_\n",
    "seed: 42\n",
    "\n",
    "data:\n",
    "  path: \"dummy\" # Path to data CSV or \"dummy\" for internal dummy data\n",
    "  item_min_count: 1 # Min interactions for an item to be kept\n",
    "  user_min_count: 1 # Min interactions for a user to be kept\n",
    "\n",
    "model:\n",
    "  name: \"Mamba4Rec\"\n",
    "  max_length: 50 # Max sequence length for padding and processing\n",
    "  padding_idx: 0\n",
    "  model_params:\n",
    "    add_head: True\n",
    "    tie_weights: True\n",
    "    init_std: 0.02\n",
    "    mamba_config: # Parameters specific to the Mamba layer itself\n",
    "      d_model: 32\n",
    "      # n_layer: 1 # The Mamba class from mamba_ssm is a single block. Stacking needs custom logic.\n",
    "      d_state: 8\n",
    "      d_conv: 2\n",
    "      expand: 2\n",
    "      # Add other mamba-specific params like bias, conv_bias if needed\n",
    "\n",
    "trainer:\n",
    "  batch_size: 16 # Reduced for small dummy data\n",
    "  num_workers: 0 # For Windows, 0 is often more stable. For Linux, can be > 0.\n",
    "  learning_rate: 1.0e-3\n",
    "  max_epochs: 3 # Keep low for quick demo\n",
    "  accelerator: \"auto\" # \"cpu\", \"gpu\", \"tpu\", \"mps\", \"auto\"\n",
    "  devices: \"auto\" # Number of devices or \"auto\"\n",
    "  deterministic: True # For reproducibility\n",
    "  early_stopping:\n",
    "    enabled: True\n",
    "    monitor: \"val_loss\" # Metric to monitor\n",
    "    patience: 3         # Number of epochs with no improvement\n",
    "    mode: \"min\"         # \"min\" for loss/error, \"max\" for accuracy/NDCG\n",
    "\n",
    "evaluation:\n",
    "  top_k_metrics: [5, 10] # List of K values for metrics\n",
    "  predict_top_k_entities: 20 # How many entities model predicts before filtering for items\n",
    "\n",
    "clearml: # Optional ClearML configuration\n",
    "  project_name: \"Mamba4Rec_Experiments\"\n",
    "  task_name: \"Default_Run\"\n",
    "\n",
    "# Hydra specific settings\n",
    "hydra:\n",
    "  run:\n",
    "    dir: outputs_mamba4rec/\\${now:%Y-%m-%d}/\\${now:%H-%M-%S}\n",
    "  sweep:\n",
    "    dir: multirun_mamba4rec/\\${now:%Y-%m-%d}/\\${now:%H-%M-%S}\n",
    "    subdir: \\${hydra.job.num}\n",
    "\n",
    "\"\"\"\n",
    "    os.makedirs(\"configs\", exist_ok=True)\n",
    "    with open(\"configs/experiment_config.yaml\", \"w\") as f:\n",
    "        f.write(config_content)\n",
    "    print(\"Dummy 'configs/experiment_config.yaml' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create a dummy config if it doesn't exist, for standalone running.\n",
    "    # In a typical Hydra project, you'd run from CLI: python your_script.py\n",
    "    if not os.path.exists(\"configs/experiment_config.yaml\"):\n",
    "        save_dummy_config()\n",
    "    \n",
    "    # Set the config path for Hydra when running as a script\n",
    "    # This is a bit of a workaround for running Hydra from a notebook-like script.\n",
    "    # Normally, Hydra finds configs based on the script's location or explicit CLI args.\n",
    "    # Here, we ensure Hydra knows where to look.\n",
    "    \n",
    "    # Launch the Hydra application\n",
    "    # Note: When running this script directly, Hydra's working directory changes.\n",
    "    # Paths in config (like data.path) might need to be absolute or relative to original script location.\n",
    "    # Alternatively, use hydra.utils.to_absolute_path() within the run_experiment function.\n",
    "    \n",
    "    # To run, you would typically execute from the command line:\n",
    "    # python <your_script_name>.py\n",
    "    # Hydra will then pick up the @hydra.main decorator and the config.\n",
    "    \n",
    "    # For interactive environments or direct script execution,\n",
    "    # you might need to adjust how Hydra is invoked or how paths are handled.\n",
    "    # The `save_dummy_config` and the call to `run_experiment()` below are\n",
    "    # primarily for making this combined script runnable.\n",
    "    \n",
    "    # If you get errors related to config path, ensure 'configs/experiment_config.yaml' exists\n",
    "    # or run with `python your_script_name.py --config-dir ./configs --config-name experiment_config`\n",
    "    # from the directory containing `your_script_name.py` and the `configs` folder.\n",
    "\n",
    "    # This direct call to run_experiment() will bypass Hydra's CLI if not run via `python script.py`\n",
    "    # For full Hydra functionality (overrides, multirun), use the CLI.\n",
    "    # However, @hydra.main should still work if the config can be loaded.\n",
    "    \n",
    "    # To make it runnable directly as `python script.py` where `script.py` is this file:\n",
    "    # We assume `configs/experiment_config.yaml` is created by `save_dummy_config`.\n",
    "    # The @hydra.main decorator will handle the rest.\n",
    "    run_experiment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
